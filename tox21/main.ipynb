{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Classification - Tox21 Dataset**\n",
    "\n",
    "A foray into all things GNN\n",
    "\n",
    "Goals:\n",
    "- Learn handle graph data (Deepchem already contains the dataset)\n",
    "- Learn how to convert SMILES to graphs\n",
    "- Learn how to run a simple GNN\n",
    "\n",
    "Let's go ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, at the outset, you (or future me) may be wondering what a \"featurizer\" is. Per DeepChem documentation: \n",
    "\n",
    "> \"\\[A] 'featurizer' is chunk of code which transforms raw input data into a processed form suitable for machine learning. Machine learning methods often need data to be pre-chewed for them to process. Think of this like a mama penguin chewing up food so the baby penguin can digest it easily.\"\n",
    "\n",
    "Fair enough.\n",
    "\n",
    "This wonderful forum post [here](https://forum.deepchem.io/t/what-is-a-featurizer/833#:~:text=In%20computer%20vision%2C%20your,That%E2%80%99s%20what%20featurization%20is) also puts it in layman's terms: \"Before you can apply machine learning to molecules, you need to decide how to represent them. Thatâ€™s what featurization is.\"\n",
    "\n",
    "In the context of the code above, `featurizer='GraphConv'` indicates that we will be using the `ConvMolFeaturizer`, which interoperates with graph convolution models that inherit `KerasModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepChem provides a classification model [`GraphConvModel`](https://github.com/deepchem/deepchem/blob/master/deepchem/models/graph_models.py) that works out of the box. \n",
    "\n",
    "Here's a high-level rundown of a few default hyperparameters:\n",
    "- width of channels for the Graph Convolution Layers = 64\n",
    "- number of tasks = 12\n",
    "- number of atom features = 75\n",
    "- dropout = 0.0\n",
    "- mode = classification\n",
    "- batch size = 100\n",
    "\n",
    "Further, it looks like we're using cross entropy loss (`SoftmaxCrossEntropy`) and ADAM optimizer (`tf.keras.optimizers.Adam`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2864856338500977"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tasks = len(tasks)\n",
    "model = dc.models.GraphConvModel(n_tasks, mode='classification')\n",
    "model.fit(train_dataset, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'roc_auc_score': 0.9686162067427923}\n",
      "Test set score: {'roc_auc_score': 0.7086946820884501}\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty accurate for 6 lines of code, but where's the fun in calling functions?\n",
    "\n",
    "Let's see if we can build on top of this model by integrating **hyperparameter optimization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in ./env/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from hyperopt) (1.24.4)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.10/site-packages (from hyperopt) (1.8.1)\n",
      "Requirement already satisfied: six in ./env/lib/python3.10/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in ./env/lib/python3.10/site-packages (from hyperopt) (3.2.1)\n",
      "Requirement already satisfied: future in ./env/lib/python3.10/site-packages (from hyperopt) (0.18.3)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.10/site-packages (from hyperopt) (4.66.1)\n",
      "Requirement already satisfied: cloudpickle in ./env/lib/python3.10/site-packages (from hyperopt) (3.0.0)\n",
      "Requirement already satisfied: py4j in ./env/lib/python3.10/site-packages (from hyperopt) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the search space, I decided to consult experts in the field (and by that I mean look at existing literature) to see what hyperparameters other groups have utilized.\n",
    "\n",
    "[Fout et al. (2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf) used the following search space during the validation stage when using GCNs for protien interface prediction:\n",
    "- Edge distance feature RBF kernel standard deviation (2 to 32)\n",
    "- Negative to positive example ratio (1:1 to 20:1)\n",
    "- Number of convolutional layers (1 to 6)\n",
    "- Number of filters (8 to 2000)\n",
    "- Neighborhood size (2 to 26)\n",
    "- Pairwise residue representation (elementwise sumproduct vs concatenation)\n",
    "- Number of dense layers after merging (0 to 4)\n",
    "- Optimization algorithm (stochastic gradient descent, RMSProp, ADAM, Momentum)\n",
    "- Learning rate (0.01 to 1)\n",
    "- Dropout probability (0.3 to 0.8)\n",
    "- Minibatch size (64 or 128 examples)\n",
    "- Number of epochs (50 to 1000) \n",
    "\n",
    "Below, I've tried to replicate the search space for hyperparameters relevant to our project. Note that I have fixed the optimizer to be ADAM. Future steps will likely involve testing different optimization algorithms.\n",
    "\n",
    "The discrete options for many hyperparameters (particularly `graph_conv_layers`) are essentially arbitrary (powers of two!); thus, yet another future step is to incorporate options that are *not* subject to the whims of the human mind. \n",
    "\n",
    "\n",
    "Another interesting finding is that \"Automatic model selection as in Bergstra et al. failed to outperform the best manual search results.\" Perhaps hyperopt may not be the *absolute* best option here, but for a proof of concept, let us proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "search_space = {\n",
    "    'graph_conv_layers': hp.choice('graph_conv_layers',[[64], [64, 64], [128, 128], [16, 64, 128, 64, 16]]),\n",
    "    'dense_layer_size': hp.choice('dense_layer_sizes', [64, 128]),\n",
    "    'dropouts': hp.uniform('dropout',low=0.1, high=0.5),\n",
    "    'batch_sizes': hp.choice('batch_sizes', [64, 100, 128]),\n",
    "    'epochs' : hp.choice('epochs', [50, 250, 750, 1000])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then declarate a function to be minimized by the hyperopt based on the structure provided by [DeepChem](https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Advanced_model_training_using_hyperopt.ipynb).\n",
    "\n",
    "For clarity, all instances of `MultitaskClassifier` in the example have been replaced with `GraphConvModel` as the latter is relevant to this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "#tempfile is used to save the best checkpoint later in the program.\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "\n",
    "def fm(args):\n",
    "  save_dir = tempfile.mkdtemp()\n",
    "  model = dc.models.GraphConvModel(\n",
    "    n_tasks=len(tasks),\n",
    "    graph_conv_layers=args['graph_conv_layers'],\n",
    "    dense_layer_size=args['dense_layer_size'],\n",
    "    dropouts=args['dropouts'],\n",
    "    mode=\"classification\",\n",
    "    number_atom_features=75,\n",
    "    n_classes=2,\n",
    "    batch_size=args['batch_sizes'],\n",
    "    batch_normalize=True,\n",
    "    uncertainty=False,\n",
    "  )\n",
    "  \n",
    "  #validation callback that saves the best checkpoint, i.e the one with the maximum score.\n",
    "  validation=dc.models.ValidationCallback(valid_dataset, 1000, [metric],save_dir=save_dir,transformers=transformers,save_on_minimum=False)\n",
    "  \n",
    "  model.fit(train_dataset, nb_epoch=25,callbacks=validation)\n",
    "\n",
    "  #restoring the best checkpoint and passing the negative of its validation score to be minimized.\n",
    "  model.restore(model_dir=save_dir)\n",
    "  valid_score = model.evaluate(valid_dataset, [metric], transformers)\n",
    "\n",
    "  return -1*valid_score['roc_auc_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.704428\n",
      " 10%|â–ˆ         | 1/10 [01:16<11:29, 76.56s/trial, best loss: -0.7044283784562052]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.721989\n",
      " 20%|â–ˆâ–ˆ        | 2/10 [01:37<05:49, 43.74s/trial, best loss: -0.7219890805136832]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.732553\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:18<04:57, 42.57s/trial, best loss: -0.7325533155548728]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1199 calls to <function KerasModel._create_gradient_fn.<locals>.apply_gradient_for_batch at 0x407ffa050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1199 calls to <function KerasModel._create_gradient_fn.<locals>.apply_gradient_for_batch at 0x407ffa050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.713442\n",
      "Step 2000 validation: roc_auc_score=0.721248\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [03:02<04:19, 43.29s/trial, best loss: -0.7325533155548728]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.741687\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [03:35<03:17, 39.50s/trial, best loss: -0.7416874582275668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.723493\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [04:11<02:32, 38.23s/trial, best loss: -0.7416874582275668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.724011\n",
      "Step 2000 validation: roc_auc_score=0.731669\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [04:53<01:58, 39.45s/trial, best loss: -0.7416874582275668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.746074\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [05:48<01:28, 44.32s/trial, best loss: -0.7460741188063159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.739122\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [06:09<00:37, 37.07s/trial, best loss: -0.7460741188063159]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 validation: roc_auc_score=0.727838\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [06:46<00:00, 40.61s/trial, best loss: -0.7460741188063159]\n"
     ]
    }
   ],
   "source": [
    "trials=Trials()\n",
    "best = fmin(fm,\n",
    "    \t\tspace= search_space,\n",
    "    \t\talgo=tpe.suggest,\n",
    "    \t\tmax_evals=10,\n",
    "    \t\ttrials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINALLY! Drumroll please. The best hyperparameters found by the hyperopt are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: {'batch_sizes': 1, 'dense_layer_sizes': 0, 'dropout': 0.3938539395554731, 'epochs': 2, 'graph_conv_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anticlimactic. Regardless, let's throw these parameters back into our model and see if we notice any improvements at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016883143186569215"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dc.models.GraphConvModel(\n",
    "    n_tasks=len(tasks),\n",
    "    graph_conv_layers=[128, 128],\n",
    "    dense_layer_size=64,\n",
    "    dropouts=0.39,\n",
    "    mode=\"classification\",\n",
    "    number_atom_features=75,\n",
    "    n_classes=2,\n",
    "    batch_size=100,\n",
    "    batch_normalize=True,\n",
    "    uncertainty=False,\n",
    ")\n",
    "model.fit(train_dataset, nb_epoch=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'roc_auc_score': 0.9936484521577092}\n",
      "Test set score: {'roc_auc_score': 0.6963760227978276}\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite adding drouput, it looks like the optimizations we made have overfit the data :(. This is certainly something to keep an eye out for. Let's proceed.\n",
    "\n",
    "Optimization has no doubt been fun, even if the results may be, uh, unexpected. We've definitely now learned how to optimize hyperparameters (in theory), so we can add it to our arsenal.\n",
    "\n",
    "But there's nothing like creating your own GNN. Let's delve into the depths of GNNs.\n",
    "\n",
    "Because Robert Frost is on my mind, why not take the road not yet taken and write our model in PyTorch rather than TensorFlow. This may require us to modify which featurizers we use among other things, but a headache *should* be worth it. I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
