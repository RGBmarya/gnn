{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepchem\n",
    "!pip install 'deepchem[torch]'\n",
    "!pip install 'deepchem[tensorflow]'\n",
    "!pip install pandas\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv', reload=False)\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, at the outset, you (or future me) may be wondering what a \"featurizer\" is. Per DeepChem documentation: \n",
    "\n",
    "> \"\\[A] 'featurizer' is chunk of code which transforms raw input data into a processed form suitable for machine learning. Machine learning methods often need data to be pre-chewed for them to process. Think of this like a mama penguin chewing up food so the baby penguin can digest it easily.\"\n",
    "\n",
    "Fair enough.\n",
    "\n",
    "This wonderful forum post [here](https://forum.deepchem.io/t/what-is-a-featurizer/833#:~:text=In%20computer%20vision%2C%20your,That%E2%80%99s%20what%20featurization%20is) also puts it in layman's terms: \"Before you can apply machine learning to molecules, you need to decide how to represent them. Thatâ€™s what featurization is.\"\n",
    "\n",
    "In the context of the code above, `featurizer='GraphConv'` indicates that we will be using the `ConvMolFeaturizer`, which interoperates with graph convolution models that inherit `KerasModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepChem provides a classification model [`GraphConvModel`](https://github.com/deepchem/deepchem/blob/master/deepchem/models/graph_models.py) that works out of the box. \n",
    "\n",
    "Here's a high-level rundown of a few default hyperparameters:\n",
    "- width of channels for the Graph Convolution Layers = 64\n",
    "- number of tasks = 12\n",
    "- number of atom features = 75\n",
    "- dropout = 0.0\n",
    "- mode = classification\n",
    "- batch size = 100\n",
    "\n",
    "Further, it looks like we're using cross entropy loss (`SoftmaxCrossEntropy`) and ADAM optimizer (`tf.keras.optimizers.Adam`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = len(tasks)\n",
    "model = dc.models.GraphConvModel(n_tasks, mode='classification')\n",
    "model.fit(train_dataset, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty accurate for 6 lines of code, but where's the fun in calling functions?\n",
    "\n",
    "Let's see if we can build on top of this model by integrating **hyperparameter optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the search space, I decided to consult experts in the field (and by that I mean look at existing literature) to see what hyperparameters other groups have utilized.\n",
    "\n",
    "[Fout et al. (2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf) used the following search space during the validation stage when using GCNs for protien interface prediction:\n",
    "- Edge distance feature RBF kernel standard deviation (2 to 32)\n",
    "- Negative to positive example ratio (1:1 to 20:1)\n",
    "- Number of convolutional layers (1 to 6)\n",
    "- Number of filters (8 to 2000)\n",
    "- Neighborhood size (2 to 26)\n",
    "- Pairwise residue representation (elementwise sumproduct vs concatenation)\n",
    "- Number of dense layers after merging (0 to 4)\n",
    "- Optimization algorithm (stochastic gradient descent, RMSProp, ADAM, Momentum)\n",
    "- Learning rate (0.01 to 1)\n",
    "- Dropout probability (0.3 to 0.8)\n",
    "- Minibatch size (64 or 128 examples)\n",
    "- Number of epochs (50 to 1000) \n",
    "\n",
    "Below, I've tried to replicate the search space for hyperparameters relevant to our project. Note that I have fixed the optimizer to be ADAM. Future steps will likely involve testing different optimization algorithms.\n",
    "\n",
    "The discrete options for many hyperparameters (particularly `graph_conv_layers`) are essentially arbitrary (powers of two!); thus, yet another future step is to incorporate options that are *not* subject to the whims of the human mind. \n",
    "\n",
    "\n",
    "Another interesting finding is that \"Automatic model selection as in Bergstra et al. failed to outperform the best manual search results.\" Perhaps hyperopt may not be the *absolute* best option here, but for a proof of concept, let us proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "search_space = {\n",
    "    'graph_conv_layers': hp.choice('graph_conv_layers',[[64], [64, 64], [128, 128], [16, 64, 128, 64, 16]]),\n",
    "    'dense_layer_size': hp.choice('dense_layer_sizes', [64, 128]),\n",
    "    'dropouts': hp.uniform('dropout',low=0.1, high=0.5),\n",
    "    'batch_sizes': hp.choice('batch_sizes', [64, 100, 128]),\n",
    "    'epochs' : hp.choice('epochs', [50, 250, 750, 1000])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then declarate a function to be minimized by the hyperopt based on the structure provided by [DeepChem](https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Advanced_model_training_using_hyperopt.ipynb).\n",
    "\n",
    "For clarity, all instances of `MultitaskClassifier` in the example have been replaced with `GraphConvModel` as the latter is relevant to this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "#tempfile is used to save the best checkpoint later in the program.\n",
    "\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "\n",
    "def fm(args):\n",
    "  save_dir = tempfile.mkdtemp()\n",
    "  model = dc.models.GraphConvModel(\n",
    "    n_tasks=len(tasks),\n",
    "    graph_conv_layers=args['graph_conv_layers'],\n",
    "    dense_layer_size=args['dense_layer_size'],\n",
    "    dropouts=args['dropouts'],\n",
    "    mode=\"classification\",\n",
    "    number_atom_features=75,\n",
    "    n_classes=2,\n",
    "    batch_size=args['batch_sizes'],\n",
    "    batch_normalize=True,\n",
    "    uncertainty=False,\n",
    "  )\n",
    "  \n",
    "  #validation callback that saves the best checkpoint, i.e the one with the maximum score.\n",
    "  validation=dc.models.ValidationCallback(valid_dataset, 1000, [metric],save_dir=save_dir,transformers=transformers,save_on_minimum=False)\n",
    "  \n",
    "  model.fit(train_dataset, nb_epoch=25,callbacks=validation)\n",
    "\n",
    "  #restoring the best checkpoint and passing the negative of its validation score to be minimized.\n",
    "  model.restore(model_dir=save_dir)\n",
    "  valid_score = model.evaluate(valid_dataset, [metric], transformers)\n",
    "\n",
    "  return -1*valid_score['roc_auc_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=Trials()\n",
    "best = fmin(fm,\n",
    "    \t\tspace= search_space,\n",
    "    \t\talgo=tpe.suggest,\n",
    "    \t\tmax_evals=10,\n",
    "    \t\ttrials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINALLY! Drumroll please. The best hyperparameters found by the hyperopt are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anticlimactic. Regardless, let's throw these parameters back into our model and see if we notice any improvements at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.GraphConvModel(\n",
    "    n_tasks=len(tasks),\n",
    "    graph_conv_layers=[128, 128],\n",
    "    dense_layer_size=64,\n",
    "    dropouts=0.39,\n",
    "    mode=\"classification\",\n",
    "    number_atom_features=75,\n",
    "    n_classes=2,\n",
    "    batch_size=100,\n",
    "    batch_normalize=True,\n",
    "    uncertainty=False,\n",
    ")\n",
    "model.fit(train_dataset, nb_epoch=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite adding drouput, it looks like the optimizations we made have overfit the data :(. This is certainly something to keep an eye out for. Let's proceed.\n",
    "\n",
    "Optimization has no doubt been fun, even if the results may be, uh, unexpected. We've definitely now learned how to optimize hyperparameters (in theory), so we can add it to our arsenal.\n",
    "\n",
    "But there's nothing like creating your own GNN. Let's delve into the depths of GNNs.\n",
    "\n",
    "Because Robert Frost is on my mind, why not take the road not yet taken and write our model in PyTorch rather than TensorFlow. This may require us to modify which featurizers we use among other things, but a headache *should* be worth it. I think.\n",
    "\n",
    "Jump to `graph_classification.ipynb` for next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
